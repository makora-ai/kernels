# Kimi Delta Attention (KDA) â€” Triton Kernels (LLM-Generated)

This repository contains **MakoraGenerate-generated Triton implementations of Kimi Delta Attention (KDA)**.  

The blog post with more detail can be found here: https://makora.com/blog/generating-kda-kernels

KDA is a simplified linear-time attention variant introduced in the *Kimi* model family.  
These Triton kernels implement the core Î”-attention update in a fused and memory-efficient way.

---

## ðŸ“ˆ Benchmark Results

| Query Ã— KeyLen Shape | Speedup vs `torch.compile` | Notes |
|----------------------|-----------------------------|-------|
| **256 Ã— 8192**       | **5.5Ã—**                    | Matches hand-optimized |
| **512 Ã— 16384**      | **7.8Ã—**                    | Matches hand-optimized |
| **8192 Ã— 1024**      | **0.34Ã—**                   | Hand-optimized kernel fails |
| **16384 Ã— 1024**     | **0.7Ã—**                    | Hand-optimized kernel fails |

All benchmarks run on the same backend using identical inputs and precision settings.
